{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import flatten\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.image as mpimg \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetParser():\n",
    "    def __init__(self):\n",
    "        self.csv_path = 'IsLotDataset/path_and_labels.csv'\n",
    "        self.IMAGE_PATH_KEY = 'image_path'\n",
    "        self.IS_LOT_KEY = 'is_lot'\n",
    "        self.split = 1000\n",
    "       \n",
    "    \n",
    "    def load(self):\n",
    "        df = self.load_origin_dataframe()\n",
    "        image_paths = df[self.IMAGE_PATH_KEY].tolist()\n",
    "        images = np.array([mpimg.imread('IsLotDataset/'+path)/255.0 for path in image_paths])\n",
    "        images = images.reshape(images.shape[0],images.shape[1],images.shape[2],1)\n",
    "        labels = np.array(df[self.IS_LOT_KEY].tolist())\n",
    "        labels = labels.reshape(len(labels),1)\n",
    "        images, labels = self.unison_shuffled_copies(images, labels)\n",
    "        onehotencoder = OneHotEncoder()\n",
    "        labels = onehotencoder.fit_transform(labels).toarray()\n",
    "        return (images[0:self.split], labels[0:self.split], images[self.split:], labels[self.split:])\n",
    "    \n",
    "    def unison_shuffled_copies(self, a, b):\n",
    "        assert len(a) == len(b)\n",
    "        p = np.random.permutation(len(a))\n",
    "        return a[p], b[p]\n",
    "\n",
    "        \n",
    "    def load_origin_dataframe(self):\n",
    "        if os.path.exists(self.csv_path):\n",
    "            df = pd.read_csv(self.csv_path, sep=',')\n",
    "            return df\n",
    "        else:\n",
    "            print('Could not load '+ORIGIN_CSV)\n",
    "            return None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = DatasetParser()\n",
    "\n",
    "(train_imgs, train_labels, test_imgs, test_labels) = parser.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "X daya Shape: (1000, 128, 128, 1)\n",
      "Image Shape: (128, 128, 1)\n",
      "Y data Shape: (1000, 2)\n",
      "Label Shape: (2,)\n",
      "\n",
      "Training Set:   1000 samples\n",
      "Test Set:       95 samples\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "print(\"X daya Shape: {}\".format(train_imgs.shape))\n",
    "print(\"Image Shape: {}\".format(train_imgs[0].shape))\n",
    "print(\"Y data Shape: {}\".format(train_labels.shape))\n",
    "print(\"Label Shape: {}\".format(train_labels[0].shape))\n",
    "print()\n",
    "print(\"Training Set:   {} samples\".format(len(train_imgs)))\n",
    "print(\"Test Set:       {} samples\".format(len(test_imgs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedLenet5():\n",
    "\n",
    "    def __init__(self, classes, learning_rate=0.0005):\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.X = tf.placeholder(tf.float32, (None, 128, 128, 1))\n",
    "        self.Y = tf.placeholder(tf.float32, [None, classes])\n",
    "        self.keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "        mu = 0\n",
    "        sigma = 0.1\n",
    "\n",
    "        #Layer 1: Convolutional. Input = 128x128x1. Output = 124x124x8.\n",
    "        conv1_w = tf.Variable(tf.truncated_normal(shape = [5,5,1,8],mean = mu, stddev = sigma))\n",
    "        conv1_b = tf.Variable(tf.zeros(8))\n",
    "        conv1 = tf.nn.conv2d(self.X,conv1_w, strides = [1,1,1,1], padding = 'VALID') + conv1_b \n",
    "        conv1 = tf.nn.relu(conv1)\n",
    "\n",
    "        #Pooling. Input = 124x124x8. Output = 62x62x8.\n",
    "        pool_1 = tf.nn.max_pool(conv1,ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'VALID')\n",
    "\n",
    "        #Layer 2: Convolutional. Output = 58x58x16.\n",
    "        conv2_w = tf.Variable(tf.truncated_normal(shape = [5,5,8,16], mean = mu, stddev = sigma))\n",
    "        conv2_b = tf.Variable(tf.zeros(16))\n",
    "        conv2 = tf.nn.conv2d(pool_1, conv2_w, strides = [1,1,1,1], padding = 'VALID') + conv2_b\n",
    "        conv2 = tf.nn.relu(conv2)\n",
    "\n",
    "        #Pooling. Input = 58x58x16. Output = 29x29x16.\n",
    "        pool_2 = tf.nn.max_pool(conv2, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'VALID') \n",
    "\n",
    "        #Flatten. Input = 13x13x16. Output = 13456.\n",
    "        fc1 = flatten(pool_2)\n",
    "\n",
    "        #Layer 3: Fully Connected. Input = 2.704. Output = 256.\n",
    "        fc1_w = tf.Variable(tf.truncated_normal(shape = (13456,256), mean = mu, stddev = sigma))\n",
    "        fc1_b = tf.Variable(tf.zeros(256))\n",
    "        fc1 = tf.matmul(fc1,fc1_w) + fc1_b\n",
    "        fc1 = tf.nn.relu(fc1)\n",
    "        fc1 = tf.nn.dropout(fc1, self.keep_prob)\n",
    "        \n",
    "\n",
    "        #Layer 4: Fully Connected. Input = 256. Output = 128.\n",
    "        fc2_w = tf.Variable(tf.truncated_normal(shape = (256,128), mean = mu, stddev = sigma))\n",
    "        fc2_b = tf.Variable(tf.zeros(128))\n",
    "        fc2 = tf.matmul(fc1,fc2_w) + fc2_b\n",
    "        fc2 = tf.nn.relu(fc2)\n",
    "        fc2 = tf.nn.dropout(fc2, self.keep_prob)\n",
    "\n",
    "        #Layer 5: Fully Connected. Input = 128. Output = 64.\n",
    "        fc3_w = tf.Variable(tf.truncated_normal(shape = (128,64), mean = mu, stddev = sigma))\n",
    "        fc3_b = tf.Variable(tf.zeros(64))\n",
    "        fc3 = tf.matmul(fc2,fc3_w) + fc3_b\n",
    "        fc3 = tf.nn.relu(fc3)\n",
    "        fc3 = tf.nn.dropout(fc3, self.keep_prob)\n",
    "\n",
    "        #Layer 6: Fully Connected. Input = 64. Output = classes.\n",
    "        fc4_w = tf.Variable(tf.truncated_normal(shape = (64,classes), mean = mu , stddev = sigma))\n",
    "        fc4_b = tf.Variable(tf.zeros(classes))\n",
    "        self.logits = tf.matmul(fc3, fc4_w) + fc4_b\n",
    "\n",
    "\n",
    "        cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=self.logits, labels=self.Y)\n",
    "        self.loss_operation = tf.reduce_mean(cross_entropy)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate = self.learning_rate)\n",
    "        self.training_operation = optimizer.minimize(self.loss_operation)\n",
    "\n",
    "        correct_prediction = tf.equal(tf.argmax(self.logits, axis=1), tf.argmax(self.Y, axis=1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "        self.session = tf.Session()\n",
    "\n",
    "\n",
    "    def fit(self, x_data, y_data, batch_size, num_epochs, x_test, y_test):\n",
    "\n",
    "        self.session.run(tf.global_variables_initializer())\n",
    "        num_iterations = int(x_data.shape[0]/batch_size)\n",
    "        historic_accuracy = np.zeros(num_epochs)\n",
    "        historic_loss = np.zeros(num_epochs)\n",
    "\n",
    "        print(\"Training...\")\n",
    "        print()\n",
    "        for epoch in range(num_epochs):\n",
    "            x_data_s, y_data_s = shuffle(x_data, y_data)\n",
    "\n",
    "            for iteration in range(num_iterations):\n",
    "                batch_xs = x_data_s[iteration*batch_size:(iteration+1)*batch_size,:]\n",
    "                batch_ys = y_data_s[iteration*batch_size:(iteration+1)*batch_size,:]\n",
    "                _ = self.session.run([self.training_operation], feed_dict={self.X: batch_xs, self.Y: batch_ys, self.keep_prob: 0.7})\n",
    "            \n",
    "            accuracy_val, loss_val = self.session.run([self.accuracy, self.loss_operation], feed_dict={self.X: batch_xs, self.Y: batch_ys, self.keep_prob: 1.0})\n",
    "            historic_accuracy[epoch] = accuracy_val\n",
    "            historic_loss[epoch] = loss_val\n",
    "            test_acc = self.compute_accuracy(x_test, y_test)\n",
    "            print('Epoch:', '%03d' % (epoch + 1), 'training acc: {}, test acc: {}, loss: {}'.format(accuracy_val, test_acc, loss_val))\n",
    "\n",
    "        print(\"Learning Finished!\")\n",
    "        return historic_accuracy, historic_loss\n",
    "\n",
    "    def predict(self, x_data):\n",
    "        output_values = self.session.run(tf.nn.softmax(self.logits, axis=1), feed_dict={self.X: x_data, self.keep_prob: 1.0})\n",
    "        return output_values\n",
    "\n",
    "    def compute_accuracy(self, x_data, y_data):\n",
    "        accuracy_val = self.session.run(self.accuracy, feed_dict={self.X: x_data, self.Y: y_data, self.keep_prob: 1.0})\n",
    "        return accuracy_val\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ensabmbler():\n",
    "\n",
    "    def __init__(self, num_models, classes, learning_rate=0.0005):\n",
    "        self.num_models = num_models\n",
    "        self.classes = classes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.models = []\n",
    "        for i in range(num_models):\n",
    "            self.models.append(ImprovedLenet5(classes=classes, learning_rate=learning_rate))\n",
    "\n",
    "    def fit(self, x_data, y_data, batch_size, num_epochs, x_test, y_test):\n",
    "        for i, model in enumerate(self.models):\n",
    "            print('-------------')\n",
    "            print('FIT MODEL {}:'.format(i+1))\n",
    "            print()\n",
    "            model.fit(x_data=x_data,\n",
    "                      y_data=y_data,\n",
    "                      batch_size=batch_size,\n",
    "                      num_epochs=num_epochs,\n",
    "                      x_test=x_test,\n",
    "                      y_test=y_test)\n",
    "            print('END FIT MODEL {}.'.format(i+1))\n",
    "            print('-------------')\n",
    "\n",
    "    def predict(self, x_data):\n",
    "        predictions = np.zeros((x_data.shape[0], self.classes))\n",
    "        for model in self.models:\n",
    "            predictions+=model.predict(x_data=x_data)\n",
    "        return predictions/self.num_models\n",
    "\n",
    "    def compute_accuracy(self, x_data, y_data):\n",
    "        predictions = self.predict(x_data=x_data)\n",
    "        correct_prediction = np.equal(np.argmax(predictions, axis=1), np.argmax(y_data, axis=1))\n",
    "        accuracy_val = float(sum(correct_prediction))/float(correct_prediction.shape[0])\n",
    "        return accuracy_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/guillemgbt/Development/Thesis/thesis_env/lib/python3.6/site-packages/tensorflow/contrib/layers/python/layers/layers.py:1634: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x1480ac780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x1480ac780>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x1480ac780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x1480ac780>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:From <ipython-input-5-8cf996f481ec>:39: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From <ipython-input-5-8cf996f481ec>:62: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x14893cef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x14893cef0>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x14893cef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x14893cef0>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x148ef29b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x148ef29b0>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x148ef29b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x148ef29b0>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x149574ac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x149574ac8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x149574ac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x149574ac8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x149c15e80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x149c15e80>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x149c15e80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x149c15e80>>: AttributeError: module 'gast' has no attribute 'Num'\n"
     ]
    }
   ],
   "source": [
    "ensambler = Ensabmbler(num_models=5, classes=2, learning_rate=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------\n",
      "FIT MODEL 1:\n",
      "\n",
      "Training...\n",
      "\n",
      "Epoch: 001 training acc: 0.7049999833106995, test acc: 0.6631578803062439, loss: 0.5961596965789795\n",
      "Epoch: 002 training acc: 0.699999988079071, test acc: 0.6421052813529968, loss: 0.5356477499008179\n",
      "Epoch: 003 training acc: 0.6949999928474426, test acc: 0.7052631378173828, loss: 0.5511245131492615\n",
      "Epoch: 004 training acc: 0.75, test acc: 0.7157894968986511, loss: 0.4847554862499237\n",
      "Epoch: 005 training acc: 0.8100000023841858, test acc: 0.75789475440979, loss: 0.4394388198852539\n",
      "Epoch: 006 training acc: 0.7900000214576721, test acc: 0.7368420958518982, loss: 0.4412446618080139\n",
      "Epoch: 007 training acc: 0.8399999737739563, test acc: 0.7789473533630371, loss: 0.3602154850959778\n",
      "Epoch: 008 training acc: 0.8349999785423279, test acc: 0.75789475440979, loss: 0.338062047958374\n",
      "Epoch: 009 training acc: 0.8700000047683716, test acc: 0.7368420958518982, loss: 0.3205970823764801\n",
      "Epoch: 010 training acc: 0.875, test acc: 0.8105263113975525, loss: 0.28036293387413025\n",
      "Epoch: 011 training acc: 0.9049999713897705, test acc: 0.8105263113975525, loss: 0.24066010117530823\n",
      "Epoch: 012 training acc: 0.8650000095367432, test acc: 0.75789475440979, loss: 0.3080684542655945\n",
      "Epoch: 013 training acc: 0.9150000214576721, test acc: 0.8105263113975525, loss: 0.21859675645828247\n",
      "Epoch: 014 training acc: 0.8949999809265137, test acc: 0.7789473533630371, loss: 0.2569389343261719\n",
      "Epoch: 015 training acc: 0.9049999713897705, test acc: 0.8315789699554443, loss: 0.23957368731498718\n",
      "Learning Finished!\n",
      "END FIT MODEL 1.\n",
      "-------------\n",
      "-------------\n",
      "FIT MODEL 2:\n",
      "\n",
      "Training...\n",
      "\n",
      "Epoch: 001 training acc: 0.6949999928474426, test acc: 0.6421052813529968, loss: 0.6036537289619446\n",
      "Epoch: 002 training acc: 0.7200000286102295, test acc: 0.7473683953285217, loss: 0.5787219405174255\n",
      "Epoch: 003 training acc: 0.7300000190734863, test acc: 0.7052631378173828, loss: 0.4964599907398224\n",
      "Epoch: 004 training acc: 0.800000011920929, test acc: 0.7368420958518982, loss: 0.4858187735080719\n",
      "Epoch: 005 training acc: 0.8050000071525574, test acc: 0.7684210538864136, loss: 0.42798709869384766\n",
      "Epoch: 006 training acc: 0.8299999833106995, test acc: 0.8105263113975525, loss: 0.3842599391937256\n",
      "Epoch: 007 training acc: 0.8399999737739563, test acc: 0.8315789699554443, loss: 0.34469863772392273\n",
      "Epoch: 008 training acc: 0.8550000190734863, test acc: 0.8421052694320679, loss: 0.32693222165107727\n",
      "Epoch: 009 training acc: 0.8799999952316284, test acc: 0.8526315689086914, loss: 0.30506062507629395\n",
      "Epoch: 010 training acc: 0.8999999761581421, test acc: 0.8631578683853149, loss: 0.2557300329208374\n",
      "Epoch: 011 training acc: 0.8500000238418579, test acc: 0.821052610874176, loss: 0.3004205822944641\n",
      "Epoch: 012 training acc: 0.8949999809265137, test acc: 0.8421052694320679, loss: 0.23563385009765625\n",
      "Epoch: 013 training acc: 0.8650000095367432, test acc: 0.8526315689086914, loss: 0.3035951554775238\n",
      "Epoch: 014 training acc: 0.9049999713897705, test acc: 0.8315789699554443, loss: 0.24600745737552643\n",
      "Epoch: 015 training acc: 0.9049999713897705, test acc: 0.8421052694320679, loss: 0.20619991421699524\n",
      "Learning Finished!\n",
      "END FIT MODEL 2.\n",
      "-------------\n",
      "-------------\n",
      "FIT MODEL 3:\n",
      "\n",
      "Training...\n",
      "\n",
      "Epoch: 001 training acc: 0.6549999713897705, test acc: 0.6421052813529968, loss: 0.6336183547973633\n",
      "Epoch: 002 training acc: 0.6899999976158142, test acc: 0.6421052813529968, loss: 0.587304949760437\n",
      "Epoch: 003 training acc: 0.6949999928474426, test acc: 0.6421052813529968, loss: 0.5596759915351868\n",
      "Epoch: 004 training acc: 0.7599999904632568, test acc: 0.6526315808296204, loss: 0.5206074118614197\n",
      "Epoch: 005 training acc: 0.6899999976158142, test acc: 0.6631578803062439, loss: 0.5184399485588074\n",
      "Epoch: 006 training acc: 0.7799999713897705, test acc: 0.7052631378173828, loss: 0.4667569100856781\n",
      "Epoch: 007 training acc: 0.824999988079071, test acc: 0.7473683953285217, loss: 0.4244614541530609\n",
      "Epoch: 008 training acc: 0.8399999737739563, test acc: 0.7684210538864136, loss: 0.35962241888046265\n",
      "Epoch: 009 training acc: 0.8600000143051147, test acc: 0.821052610874176, loss: 0.3390463888645172\n",
      "Epoch: 010 training acc: 0.8349999785423279, test acc: 0.8315789699554443, loss: 0.3432791233062744\n",
      "Epoch: 011 training acc: 0.8550000190734863, test acc: 0.800000011920929, loss: 0.3620304763317108\n",
      "Epoch: 012 training acc: 0.8600000143051147, test acc: 0.8105263113975525, loss: 0.3129580318927765\n",
      "Epoch: 013 training acc: 0.8899999856948853, test acc: 0.8105263113975525, loss: 0.25829264521598816\n",
      "Epoch: 014 training acc: 0.8849999904632568, test acc: 0.8105263113975525, loss: 0.2559375762939453\n",
      "Epoch: 015 training acc: 0.9049999713897705, test acc: 0.821052610874176, loss: 0.25577855110168457\n",
      "Learning Finished!\n",
      "END FIT MODEL 3.\n",
      "-------------\n",
      "-------------\n",
      "FIT MODEL 4:\n",
      "\n",
      "Training...\n",
      "\n",
      "Epoch: 001 training acc: 0.7149999737739563, test acc: 0.6421052813529968, loss: 0.549157440662384\n",
      "Epoch: 002 training acc: 0.675000011920929, test acc: 0.6842105388641357, loss: 0.5657336711883545\n",
      "Epoch: 003 training acc: 0.7549999952316284, test acc: 0.7052631378173828, loss: 0.47456490993499756\n",
      "Epoch: 004 training acc: 0.8199999928474426, test acc: 0.75789475440979, loss: 0.41993531584739685\n",
      "Epoch: 005 training acc: 0.824999988079071, test acc: 0.8421052694320679, loss: 0.40829354524612427\n",
      "Epoch: 006 training acc: 0.800000011920929, test acc: 0.7894737124443054, loss: 0.41644054651260376\n",
      "Epoch: 007 training acc: 0.8299999833106995, test acc: 0.7789473533630371, loss: 0.36954978108406067\n",
      "Epoch: 008 training acc: 0.8500000238418579, test acc: 0.8421052694320679, loss: 0.3346666395664215\n",
      "Epoch: 009 training acc: 0.8899999856948853, test acc: 0.8631578683853149, loss: 0.2670424282550812\n",
      "Epoch: 010 training acc: 0.8899999856948853, test acc: 0.8631578683853149, loss: 0.26677340269088745\n",
      "Epoch: 011 training acc: 0.875, test acc: 0.8421052694320679, loss: 0.26122307777404785\n",
      "Epoch: 012 training acc: 0.9350000023841858, test acc: 0.8421052694320679, loss: 0.1926863044500351\n",
      "Epoch: 013 training acc: 0.9200000166893005, test acc: 0.8315789699554443, loss: 0.1945783942937851\n",
      "Epoch: 014 training acc: 0.949999988079071, test acc: 0.8315789699554443, loss: 0.13148848712444305\n",
      "Epoch: 015 training acc: 0.9649999737739563, test acc: 0.8421052694320679, loss: 0.11682186275720596\n",
      "Learning Finished!\n",
      "END FIT MODEL 4.\n",
      "-------------\n",
      "-------------\n",
      "FIT MODEL 5:\n",
      "\n",
      "Training...\n",
      "\n",
      "Epoch: 001 training acc: 0.6700000166893005, test acc: 0.6631578803062439, loss: 0.5900049805641174\n",
      "Epoch: 002 training acc: 0.7300000190734863, test acc: 0.7052631378173828, loss: 0.5362939238548279\n",
      "Epoch: 003 training acc: 0.7699999809265137, test acc: 0.7263157963752747, loss: 0.4719570577144623\n",
      "Epoch: 004 training acc: 0.8100000023841858, test acc: 0.7473683953285217, loss: 0.4400847554206848\n",
      "Epoch: 005 training acc: 0.7950000166893005, test acc: 0.7684210538864136, loss: 0.44005584716796875\n",
      "Epoch: 006 training acc: 0.8450000286102295, test acc: 0.800000011920929, loss: 0.36317169666290283\n",
      "Epoch: 007 training acc: 0.8700000047683716, test acc: 0.821052610874176, loss: 0.32037365436553955\n",
      "Epoch: 008 training acc: 0.8349999785423279, test acc: 0.821052610874176, loss: 0.32840684056282043\n",
      "Epoch: 009 training acc: 0.8899999856948853, test acc: 0.8526315689086914, loss: 0.25383827090263367\n",
      "Epoch: 010 training acc: 0.8849999904632568, test acc: 0.8315789699554443, loss: 0.27138081192970276\n",
      "Epoch: 011 training acc: 0.9200000166893005, test acc: 0.8421052694320679, loss: 0.21461692452430725\n",
      "Epoch: 012 training acc: 0.9350000023841858, test acc: 0.8315789699554443, loss: 0.22822017967700958\n",
      "Epoch: 013 training acc: 0.8999999761581421, test acc: 0.8421052694320679, loss: 0.20595623552799225\n",
      "Epoch: 014 training acc: 0.8999999761581421, test acc: 0.821052610874176, loss: 0.18197289109230042\n",
      "Epoch: 015 training acc: 0.9649999737739563, test acc: 0.8315789699554443, loss: 0.13136422634124756\n",
      "Learning Finished!\n",
      "END FIT MODEL 5.\n",
      "-------------\n"
     ]
    }
   ],
   "source": [
    "ensambler.fit(x_data=train_imgs,\n",
    "              y_data=train_labels,\n",
    "              batch_size=200,\n",
    "              num_epochs=15,\n",
    "              x_test=test_imgs,\n",
    "              y_test=test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8105263157894737"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensambler.compute_accuracy(x_data=test_imgs, y_data=test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speed_test(model, x):\n",
    "    time1 = time.time()\n",
    "    [model.predict(image.reshape(1,image.shape[0],image.shape[1],image.shape[2])) for image in x]\n",
    "    enlapsed = (time.time()-time1)*1000\n",
    "    print('Test took {:.3f} ms to predict {} images'.format(enlapsed, len(x)))\n",
    "    print('Single inference took {:.3f} ms'.format(enlapsed/len(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test took 46891.415 ms to predict 95 images\n",
      "Single inference took 493.594 ms\n"
     ]
    }
   ],
   "source": [
    "speed_test(ensambler, x=test_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
